---
title: "grobid_version_testing"
format: html
---

# Testing out different Grobid versions

### Loading the relevant packages, setting up variables

```{r}
# --- Configuration ---
PDF_SOURCE_DIR <- '~/Nextcloud/papercheck/pdfs/psych_science_250_oa'
OUTPUT_DIR <- '~/Nextcloud/papercheck/grobid_xmls/psych_science_250_oa'
NTFY_URL <- 'https://ntfy.jakubwerner.com/papercheck'
CORRIGENDUM_PAPERS_START_DICT <- list( # maps for which papers we have to specify start=X
    "0956797617710785.pdf" = 6,
    "0956797618795679.pdf" = 2,
    "0956797618815482.pdf" = 2,
    "0956797619830329.pdf" = 2,
    "0956797620959594.pdf" = 4
)
GROBID_VERSION_TO_ENDPOINT <- list(
    "0.8.2-delft" = "http://devserver.panda-pythagorean.ts.net:8070/",
    "0.8.2-crf" = "http://devserver.panda-pythagorean.ts.net:8072/",
    "0.8.3-SNAPSHOT-20250914-delft" = "http://devserver.panda-pythagorean.ts.net:8074/",
    "0.8.3-SNAPSHOT-20250914-crf" = "http://devserver.panda-pythagorean.ts.net:8076/"
)

library(papercheck)
```

### Now, convert the PDFs with pdf2grobid into distinct subfolders for each of the grobid versions

### First, without consolidation of citations, headers, and funders

```{r}

pdf_files <- list.files(PDF_SOURCE_DIR, pattern = "\\.pdf$", full.names = TRUE)
start_values <- CORRIGENDUM_PAPERS_START_DICT
# for all other papers, start = 1

for (grobid_version in names(GROBID_VERSION_TO_ENDPOINT)) {
    grobid_endpoint <- GROBID_VERSION_TO_ENDPOINT[[grobid_version]]
    file_path_unconsolidated <- file.path(OUTPUT_DIR, paste0(grobid_version, "_unconsolidated"))
    dir.create(file_path_unconsolidated, showWarnings = FALSE, recursive = TRUE)
    file_path_consolidated <- file.path(OUTPUT_DIR, paste0(grobid_version, "_consolidated_glutton_v1"))
    dir.create(file_path_consolidated, showWarnings = FALSE, recursive = TRUE)
    # let's process all the files in the PDF_SOURCE_DIR
    flush.console()
    print(paste("Processing", length(pdf_files), "files with Grobid version", grobid_version))
    for (pdf_file in pdf_files) {
        print(paste("Processing file:", pdf_file))
        flush.console()
        pdf_basename <- basename(pdf_file)
        xml_basename <- sub("\\.pdf$", ".xml", pdf_basename)
        xml_path_unconsolidated <- file.path(file_path_unconsolidated, xml_basename)
        xml_path_consolidated <- file.path(file_path_consolidated, xml_basename)
        
        print(paste("Processing unconsolidated XML for file:", pdf_basename))
        if (!file.exists(xml_path_unconsolidated)) {
            papercheck::pdf2grobid(pdf_file, save_path = file_path_unconsolidated,
                               grobid_url = grobid_endpoint,
                               start = start_values[[pdf_basename]],
                               end = -1,
                               consolidateCitations = 0,
                               consolidateHeader = 0,
                               consolidateFunders = 0)
        } else {
            print("Unconsolidated XML already exists, skipping.")
        }
        
        print(paste("Processing consolidated XML for file:", pdf_basename))
        if (!file.exists(xml_path_consolidated)) {
            papercheck::pdf2grobid(pdf_file, save_path = file_path_consolidated,
                               grobid_url = grobid_endpoint,
                               start = start_values[[pdf_basename]],
                               end = -1,
                               consolidateCitations = 1,
                               consolidateHeader = 1,
                               consolidateFunders = 1
                               )
        } else {
            print("Consolidated XML already exists, skipping.")
        }
        flush.console()
    }
    httr::POST(NTFY_URL, body = paste("Finished processing with Grobid version", grobid_version))
}

```

```{r}
library(ollamar) # will help us determine differences between XMLs
```

```{r}
psychsci = papercheck::read('~/Nextcloud/papercheck/grobid_xmls/psych_science_250_oa/0.8.3-SNAPSHOT-20250914-delft_consolidated_glutton_v1/0956797614527830.xml')

papercheck::info_table(psychsci, c("title", "keywords", "doi", "description"))
```

```{r}
papercheck::author_table(psychsci)
```

```{r}
psychsci$bib
```

```{r}
psychsci$xrefs
```

```{r}
papercheck::search_text(psychsci, return = "div")
```

```{r}
source("scripts/create_paper_table.R")

paper <- papercheck::read("~/Nextcloud/papercheck/grobid_xmls/psych_science_250_oa/0.8.3-SNAPSHOT-20250914-delft_consolidated_glutton_v1/0956797614527830.xml")

pt = create_paper_table(paper)


pt

# --- Build paper tables for every Grobid version & paper ---
# Requires that the XMLs have already been generated in the earlier chunk.

source("scripts/create_paper_table.R")

timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
all_tables <- list()

consolidation_modes <- c("unconsolidated", "consolidated_glutton_v1")

message("Collecting XML outputs to build paper tables...")
for (grobid_version in names(GROBID_VERSION_TO_ENDPOINT)) {
    for (mode in consolidation_modes) {
        xml_dir <- file.path(OUTPUT_DIR, paste0(grobid_version, "_", mode))
        if (!dir.exists(xml_dir)) {
            message("Directory does not exist (skipping): ", xml_dir)
            next
        }
        xml_files <- list.files(xml_dir, pattern = "\\.xml$", full.names = TRUE)
        if (length(xml_files) == 0) {
            message("No XML files in ", xml_dir, ", skipping.")
            next
        }
        message("Processing ", length(xml_files), " XML files for version ", grobid_version, " (", mode, ") ...")
        for (xf in xml_files) {
            # Robust read with retry (in case of transient IO issues)
            attempt <- 1
            paper <- NULL
            while (attempt <= 2) { # up to two attempts
                paper <- tryCatch(papercheck::read(xf), error = function(e) e)
                if (!inherits(paper, "error")) break
                message("Read failed (attempt ", attempt, ") for ", xf, ": ", paper$message)
                Sys.sleep(0.5)
                attempt <- attempt + 1
            }
            if (inherits(paper, "error")) {
                message("Giving up on ", xf)
                next
            }
            tbl <- tryCatch(create_paper_table(paper), error = function(e) e)
            if (inherits(tbl, "error")) {
                message("create_paper_table failed for ", xf, ": ", tbl$message)
                next
            }
            # Ensure it's a data.frame / tibble
            if (!inherits(tbl, c("data.frame", "tbl_df", "tbl"))) {
                message("Unexpected table type for ", xf, ", coercing to data.frame")
                tbl <- as.data.frame(tbl)
            }
            tbl$grobid_version <- grobid_version
            tbl$consolidation_mode <- mode
            tbl$pdf_filename <- sub("\\.xml$", ".pdf", basename(xf))
            all_tables[[length(all_tables) + 1]] <- tbl
        }
    }
}

if (length(all_tables) == 0) {
    warning("No tables were generated. Nothing to save.")
} else {
    suppressWarnings({
        suppressMessages({
            if (!requireNamespace("dplyr", quietly = TRUE)) {
                message("Package 'dplyr' not available; attempting to bind via base::do.call.")
                final_table <- do.call(rbind, all_tables)
            } else {
                final_table <- dplyr::bind_rows(all_tables)
            }
        })
    })
    save_rds_path <- file.path(OUTPUT_DIR, paste0("paper_tables_", timestamp, ".rds"))
    saveRDS(final_table, save_rds_path)
    message("Saved RDS: ", save_rds_path)
    if (requireNamespace("readr", quietly = TRUE)) {
        csv_path <- file.path(OUTPUT_DIR, paste0("paper_tables_", timestamp, ".csv"))
        readr::write_csv(final_table, csv_path)
        message("Saved CSV: ", csv_path)
    } else {
        message("Package 'readr' not installed; skipping CSV export.")
    }

    # Quick preview
    message("Preview of combined table (first 5 rows):")
    print(utils::head(final_table, 5))
}
```

\`\`\`
